{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6FRJRM2itz7"
      },
      "source": [
        "# For array operations\r\n",
        "import numpy as np\r\n",
        "# For Data Visualization\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# For Data Manipulation using Dataframes\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15DEKUaHkDSQ"
      },
      "source": [
        "dataset = pd.read_csv('Data.csv')\r\n",
        "# Features (all data excluding target variable in last column)\r\n",
        "X = dataset.iloc[:, :-1].values\r\n",
        "# Target/Dependent variable (assuming last column)\r\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtVU_Dip0rp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1081fde-a393-4416-f745-93b12c2cca28"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SVTks7up2ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d398ff1f-a2f4-4f86-bfe0-0bf1e59c194a"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRlmPQ3taTJ"
      },
      "source": [
        "If missing data acounts for less than 1% of dataset, we can discard them. But in all other cases, we have to replace missing data. Missing data can be replaced with either mean, median, most frequent data or with a constant using `SimpleImputer` from `sklearn.impute`. Other solutions include `IterativeImputer`, `KNNImputer` and `MissingIndicator`.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPPY_JpOyIz_"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\r\n",
        "# Replace missing values with mean\r\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\r\n",
        "# First fit, then transform for replacing missing values\r\n",
        "# Only columns with numerical values should be considered for performing mean\r\n",
        "imputer.fit(X[:, 1:3])\r\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPX1IGHa24tU",
        "outputId": "e79a373f-5e98-432b-d13c-873fa1bd1201"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz4wClFn89sx"
      },
      "source": [
        "# One-hot encoding of 'Country' column\r\n",
        "# Replace 'Country' column with 3 new columns (since we have 3 categories for 'Country')\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\r\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVd_7fGhCUSt",
        "outputId": "76573fd6-b3e9-40b7-a00f-a70da2313080"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7448gz_K9Aab"
      },
      "source": [
        "# Replace 'Purchased' column values with 0's and 1's (One-hot encoding not needed since only two categories)\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "le = LabelEncoder()\r\n",
        "y = le.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rEW5QKdEZkh",
        "outputId": "f910e646-ff39-40c2-91db-e9abd9e638b4"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlicCMQTJ55r"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9LTmew4Kfar",
        "outputId": "8290ce2d-36c7-4cd6-ceea-eaa974846af6"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmzFRKDHKh6b",
        "outputId": "c32f48b4-1920-4b04-8446-77b70149664a"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQwA2T-2Kl7l",
        "outputId": "2393e746-24c4-4fd0-8874-1104da3e5bf3"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re6MlZGtKnss",
        "outputId": "e575b5db-0619-4064-a6ae-f1459ba7cba0"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksLer_qYwQx8"
      },
      "source": [
        "Feature Scalling is performed only when different features are not scaled uniformly i.e. some features dominate other features so that there is a possibility for the machine learning model to discard the least dominant features. It is applicable only for certain machine learning models which do not inherently take care of the Feature Scaling problem.\r\n",
        "\r\n",
        "For example, Linear Regression Models inherently take care of the Feature Scaling since Target Variable can be expressed as an explicit equation in terms of the features. In that scenario, coefficients compensate for high degree of variation between different features. But models (for example, Support Vector Regression) that relate Target Variable to Features as an implicit relationship need Feature Scaling before training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkTMx7PmJWMp"
      },
      "source": [
        "Feature Scaling can be done either using Standardization or using Normalization.\r\n",
        "\r\n",
        "1.   Standardization scales the features to lie between, say -2 and +2. It is applicable in almost all scenarios.\r\n",
        "2.   Normalization scales the features to lie between 0 and 1. It is applicable in scenarios where most of the features follow a Normal Distribution.\r\n",
        "\r\n",
        "![Feature Scaling - Standardization Vs Normalization](Feature-Scaling-Standardization-Vs-Normalization.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvD2BUCfyAdI"
      },
      "source": [
        "Since Standardization is preferred generally, we are going for Feature Scaling based on Standardization. Since our one-hot encoded `country` columns take values 0 and 1 (already lie between -2 and +2), we don't need to apply Feature Scaling there since it can change those values and create problems with interpretability of one-hot encoded columns. Also, since we have encoded the Target Variable `Purchased` column using `LabelEncoder` that takes values 0 and 1, we don't need to apply Feature Scaling there also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOJfeZlHNMBY"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "# Don't apply Feature Scaling to dummy variables (that replaced 'Country' column) to keep interpretability of the model\r\n",
        "sc = StandardScaler()\r\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\r\n",
        "# Only need to transform X_test according to feature scaling applied on X_train\r\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ekg74t7Vt_H",
        "outputId": "06cdd818-4ad5-479e-e3b8-2a7c0b49708c"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDsXFRErVxTq",
        "outputId": "c4843cbf-a1e8-4581-8709-5c3136ce70e3"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}