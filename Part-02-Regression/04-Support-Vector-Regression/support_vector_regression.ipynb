{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "support_vector_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3PAEPRDRLA3"
      },
      "source": [
        "# Support Vector Regression (SVR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7b8JSF7Adq"
      },
      "source": [
        "## Intuition behind Support Vector Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUYA2jgQ3Ri3"
      },
      "source": [
        "Instead of finding the best fitting line as in Linear Regression, in SVR we focus on finding the best fitting tube. The tube is actually a regression line with an error $\\epsilon$ that we disregard. Thus, we create an $\\epsilon$-insensitive tube, disregard errors due to all the points within the tube and instead focus on points outside the tube. We have to find the narrowest tube with minimum prediction error for the training data. We fit the tube such that these two conditions are satisfied:\r\n",
        "\r\n",
        "1.   the width of the tube is a minimum\r\n",
        "2.   the sum of the errors due to outer points (called \"Slack Variables\" $\\xi_i$ and $\\xi_i^*$) is a minimum.\r\n",
        "\r\n",
        "![Support Vector Regression Intuition](Support-Vector-Regression-Intuition-01.PNG)\r\n",
        "\r\n",
        "All points in this space can be treated as vectors. Since points corresponding to Slack Variables are supporting or dictating the structure or formation of this tube, such vectors are called Support Vectors and the regression method is called Support Vector Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTPro5hPfn6o"
      },
      "source": [
        "**Important Note:** Here we have discussed Linear Support Vector Regression Model. But SVR can be applied to non-linear cases also. Non-linear SVR is a bit complex and will be discussed in the future once we get a hold of SVM (Support Vector Machine) and Kernel SVM sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOl9H2fNvvSN"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7ZZS1r4gsN"
      },
      "source": [
        "We have a dataset containing salary data of employees of different position levels (1 to 10) in a company. Here,\r\n",
        "\r\n",
        "\r\n",
        "*   Independent Variable/Feature = Level\r\n",
        "*   Dependent/Target Variable = Salary\r\n",
        "\r\n",
        "Here, Salary and Level have non-linear relationship between them. We want to build a model that can predict the salary given the level of an employee. Since we have a non-linear dataset, this problem can be formulated as a Polynomial Linear Regression problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VCUAVIjRdzZ"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLc7dTeSy0_9"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXVXoFWtSF4_"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My8xvmjFy5sK"
      },
      "source": [
        "dataset = pd.read_csv('Position_Salaries.csv')\r\n",
        "X = dataset.iloc[:, 1:-1].values\r\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYD_wROl0kU_"
      },
      "source": [
        "Here we have no missing data in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRlmPQ3taTJ"
      },
      "source": [
        "**Important Note:** If missing data acounts for less than 1% of dataset, we can discard them. But in all other cases, we have to replace missing data. Missing data can be replaced with either mean, median, most frequent data or with a constant using `SimpleImputer` from `sklearn.impute`. Other solutions include `IterativeImputer`, `KNNImputer` and `MissingIndicator`.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadrvE7s_lS9"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuwnPMCU7fHk"
      },
      "source": [
        "In the dataset, you can see that 'Position' column is effectively encoded in 'Level' column. Hence, 'Position' column is redundant and is not included in the Feature Matrix X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKEbUju49M__"
      },
      "source": [
        "Here, we want to predict the salary for Employee Level (which is a continuous real value that varies from 1 to 10). Hence we want the data for all levels from 1 to 10 for accurate prediction. Hence, in this scenario, we are not splitting the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8FeLHYS-nI"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50_Jb5qgOOkQ"
      },
      "source": [
        "In Linear Regression Models, even if some features have higher values than others, the coefficients will compensate to put every feature on the same scale. But for Support Vector Regression Models, there is no explicit relationship between Features and Target Variable that can be represented using coefficients. Hence, Feature Scaling is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo1Ox5LWzJBg"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "sc = StandardScaler()\r\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiU6D2QFRjxY"
      },
      "source": [
        "## Training the SVR model on the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deDnDr8UR5vq"
      },
      "source": [
        "## Predicting a new result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzedFlUISSu_"
      },
      "source": [
        "## Visualising the SVR results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UahPVNlJSZ-K"
      },
      "source": [
        "## Visualising the SVR results (for higher resolution and smoother curve)"
      ]
    }
  ]
}