{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random_forest_regression.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeamvpPVXuS_"
      },
      "source": [
        "# Random Forest Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7b8JSF7Adq"
      },
      "source": [
        "## Intuition behind Random Forest Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivIZvKadD9f3"
      },
      "source": [
        "Random Forest Regression is a version of Ensemble Learning (other versions include Gradient Boosting, Stacking etc.). In Ensemble Learning, you take multiple algorithms or same algorithm multiple times and put them together to make something much more powerful than the original ones.\r\n",
        "\r\n",
        "In Decision Tree Regression, prediction is done based on a single tree. But in Random Forest Regression, prediction is done based on a forest of trees. The steps for Random Forest Regression are as below:\r\n",
        "\r\n",
        "![Random Forest Regression - Intuition](Random-Forest-Regression-Intuition-01.PNG)\r\n",
        "\r\n",
        "This method has 2 inherent advantages:\r\n",
        "\r\n",
        "1.   Since we are taking average across a forest of trees, accuracy is improved. Even if we have some bad trees in the forest, accuracy will not be affected much.\r\n",
        "2.   Ensemble Learning models have high stability i.e. even if there are some changes in the dataset that may affect several trees, it will not really impact the forest as a whole.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOl9H2fNvvSN"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7ZZS1r4gsN"
      },
      "source": [
        "We have a dataset containing salary data of employees of different position levels (1 to 10) in a company. Here,\r\n",
        "\r\n",
        "\r\n",
        "*   Independent Variable/Feature = Level\r\n",
        "*   Dependent/Target Variable = Salary\r\n",
        "\r\n",
        "Here, Salary and Level have non-linear relationship between them. We want to build a model that can predict the salary given the level of an employee.\r\n",
        "\r\n",
        "**Important Note:** Decision Tree Regression works well with complex datasets having multiple features. But it is really not the best adapted to 2D datasets with only one Feature and one Target Variable. Here, we are using a simple dataset with only one feature in order to visualize both X and y in a 2D plot. Hence this dataset is not ideal for visualizing the merits of Decision Tree Regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2wvZ7SKXzVC"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVmESEFZX4Ig"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgbK_F8-X7em"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adBE4tjQX_Bh"
      },
      "source": [
        "dataset = pd.read_csv('Position_Salaries.csv')\n",
        "X = dataset.iloc[:, 1:-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYD_wROl0kU_"
      },
      "source": [
        "Here we have no missing data in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRlmPQ3taTJ"
      },
      "source": [
        "**Important Note:** If missing data acounts for less than 1% of dataset, we can discard them. But in all other cases, we have to replace missing data. Missing data can be replaced with either mean, median, most frequent data or with a constant using `SimpleImputer` from `sklearn.impute`. Other solutions include `IterativeImputer`, `KNNImputer` and `MissingIndicator`.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadrvE7s_lS9"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25WpFX0fUho"
      },
      "source": [
        "If any of the columns have categorical data, you should apply Encoding to convert them into numerical data. Encoding should be\r\n",
        "\r\n",
        "*   One Hot Encoding if you know there is no ordered relationship in your categorical variable (eg: Country, State etc.)\r\n",
        "*   Label Encoding if there is an ordered relationship (eg: Position Levels in a company, Purchase Decisions etc.)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZqVOmavjptP"
      },
      "source": [
        "In the dataset, you can see that 'Position' column is effectively encoded in 'Level' column. Thus, no explicit encoding is required. Also, since 'Position' column is redundant, it is not included in the Feature Matrix X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKEbUju49M__"
      },
      "source": [
        "Here, we want to predict the salary for Employee Level (which is a continuous real value that varies from 1 to 10). Hence we want the data for all levels from 1 to 10 for accurate prediction. Hence, in this scenario, we are not splitting the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8FeLHYS-nI"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db3wDMTl8eSg"
      },
      "source": [
        "Since predictions from Decision Tree Regression Model or Random Forest Regression Model are resulting from successive splits of the dataset, they both don't require Feature Scaling. Hence, it is not applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4S2fyIBYDcu"
      },
      "source": [
        "## Training the Random Forest Regression model on the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IbsXbK3YM4M"
      },
      "source": [
        "## Predicting a new result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLqF9yMbYTon"
      },
      "source": [
        "## Visualising the Random Forest Regression results (higher resolution)"
      ]
    }
  ]
}