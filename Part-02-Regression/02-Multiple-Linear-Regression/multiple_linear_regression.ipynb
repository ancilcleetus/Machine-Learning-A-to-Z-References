{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiple_linear_regression.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazISR8X_HUG"
      },
      "source": [
        "# Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S57a5lhfiL_r"
      },
      "source": [
        "## Assumptions of Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZz_Nb9MiYUA"
      },
      "source": [
        "![Assumptions of Linear Regression](Multiple-Linear-Regression-Intuition-00.PNG)\r\n",
        "\r\n",
        "We have to check whether these assumptions hold for our dataset before going with a Linear Regression Model. But checking these for a big dataset is time consuming. Instead, you can choose to go with a Linear Regression Model and if it gives lower accuracy than other models, then we can safely conclude that Linear Regression Models cannnot be used for this particular dataset. As experimenting with different Machine Learning Models doesn't take much time, thanks to the open-source libraries, this is preferred instead of going behind the assumptions early on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7b8JSF7Adq"
      },
      "source": [
        "## Intuition behind Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUYA2jgQ3Ri3"
      },
      "source": [
        "Linear Regression is useful when we want to predict a continuous numerical value for linear datasets (i.e. datasets with linear relationships between Features and Target Variable). Depending on the no: of features, Linear Regression can be\r\n",
        "\r\n",
        "1.   Simple Linear Regression - only a single feature\r\n",
        "2.   Multiple Linear Regression - multiple features\r\n",
        "\r\n",
        "\r\n",
        "Multiple Linear Regression involves finding the best fitting line that correlates multiple features with the target variable.\r\n",
        "![Multiple Linear Regression Equation](Multiple-Linear-Regression-Intuition-01.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOl9H2fNvvSN"
      },
      "source": [
        "### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7ZZS1r4gsN"
      },
      "source": [
        "Lets say we have a dataset containing profit data of 50 startups with vastly different proportional expenditure in R&D, Administration and Marketing. All of these are situated in one of the 3 states - New York, California or Florida. Here,\r\n",
        "\r\n",
        "\r\n",
        "*   Independent Variable/Feature = R&D Spend, Administration Spend, Marketing Spend, State\r\n",
        "*   Dependent/Target Variable = Profit\r\n",
        "\r\n",
        "The given data is to be analyzed for a Venture Capitalist Fund to find if there is any correlation between the Profit, the different amounts spent in different heads (R&D, Administration, Marketing) and the state where the Startup is situated. Since we have multiple features and the Target Variable (Profit) is a continuous variable, this is a Multiple Linear Regression problem.\r\n",
        "\r\n",
        "How this scenario fits into Multiple Linear Regression can be visualized as below:\r\n",
        "![Multiple Linear Regression for Profit Prediction: Problem](Multiple-Linear-Regression-Intuition-02.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry_BWxYUwCm1"
      },
      "source": [
        "### Dummy Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwy5ZxoLv_J1"
      },
      "source": [
        "As you can see, we encounter difficulty in mapping 'State' column into the Multiple Linear Regression Equation. We first need to convert such categorical columns into numerical values using **Dummy Variables**. One-hot Encoding can be used for achieving that.\r\n",
        "\r\n",
        "Suppose we have only two States, New York and California. Then we can apply Dummy Variables as below:\r\n",
        "![Multiple Linear Regression for Profit Prediction: Dummy Variables](Multiple-Linear-Regression-Intuition-03.PNG)\r\n",
        "\r\n",
        "Only one Dummy Column needs to be retained in this case since it represents our State info fully.\r\n",
        "\r\n",
        "$D_1 = 0 : State = California$\r\n",
        "\r\n",
        "$D_1 = 1 : State = New York$\r\n",
        "\r\n",
        "The phenomenon where one or several independent variables in a Linear Regression Model predict another is called **Multicollinearity**. If we use two dummy variables $D_1$ and $D_2$, we fall into this **Dummy Variable Trap**. As a general rule, always omit one dummy variable irrespective of the no: of dummy variables. Also, if you have two sets of dummy variables, then you need to apply the same rule to each set.\r\n",
        "\r\n",
        "![Multiple Linear Regression for Profit Prediction: Dummy Variable Trap](Multiple-Linear-Regression-Intuition-04.PNG)\r\n",
        "\r\n",
        "But, in our dataset, we have three categories for 'State' column - New York, California and Florida. Hence we need to use only two Dummy Variables and discard one. We don't need to worry about the **Dummy Variable Trap**, since it will be automatically taken cared by *Linear Regression Class*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Tts4YK9alp"
      },
      "source": [
        "### Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtu7lP_K-Nw4"
      },
      "source": [
        "Our requirement is to build a model that best predicts the general trend from the dataset. Hence, we need to choose the variables carefully so as not to fall into the trap of $Overfitting$ or $Underfitting$.\r\n",
        "\r\n",
        "![Multiple Linear Regression for Profit Prediction: Building a Model](Multiple-Linear-Regression-Intuition-05.PNG)\r\n",
        "\r\n",
        "The different methods for building a model are as below:\r\n",
        "\r\n",
        "![Multiple Linear Regression for Profit Prediction: Methods for Building a Model](Multiple-Linear-Regression-Intuition-06.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j08Hv_bKDBLs"
      },
      "source": [
        "#### Method 1: All-in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2CsstnCJPM"
      },
      "source": [
        "This is applicable if you are sure that you have to use all the independent variables. This can be the scenario in the following cases:\r\n",
        "\r\n",
        "![Multiple Linear Regression - Building a Model: All-in Method](Multiple-Linear-Regression-Intuition-07.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA1dZFLxEqGX"
      },
      "source": [
        "#### Method 2: Backward Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n99Gf_wHEy0Q"
      },
      "source": [
        "Backward Elimination Method is the approach that yields results fast. The steps for Backward Elimination Method are as below:\r\n",
        "\r\n",
        "![Multiple Linear Regression - Building a Model: Backward Elimination Method](Multiple-Linear-Regression-Intuition-08.PNG)\r\n",
        "\r\n",
        "**Important Note:** Each time a variable is removed, the model should be rebuilt with new coefficients and constants in Step 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at4T2dHuMdrQ"
      },
      "source": [
        "#### Method 3: Forward Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVnhH4_zMi7U"
      },
      "source": [
        "The steps for Forward Selection Method are as below:\r\n",
        "\r\n",
        "![Multiple Linear Regression - Building a Model: Forward Selection Method](Multiple-Linear-Regression-Intuition-09.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu6ZmuLcZcwo"
      },
      "source": [
        "#### Method 4: Bidirectional Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd41smGoZeMf"
      },
      "source": [
        "The steps for Bidirectional Elimination Method are as below:\r\n",
        "\r\n",
        "![Multiple Linear Regression - Building a Model: Bidirectional Elimination Method](Multiple-Linear-Regression-Intuition-10.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWab7gW0Z7Ih"
      },
      "source": [
        "#### Method 5: Score Comparison of All Possible Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yaL_xjTZ8ET"
      },
      "source": [
        "The steps for Score Comparison Method are as below:\r\n",
        "\r\n",
        "![Multiple Linear Regression - Building a Model: Score Comparison Method](Multiple-Linear-Regression-Intuition-11.PNG)\r\n",
        "\r\n",
        "**Important Note:** Here, we do not try to remove any non-significant columns. But, given a dataset, we go all in for all the possible models and then find the best one. Hence, it is a very resource consuming approach and is not recommended for datasets with a large no: of columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOyqYHTk_Q57"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF5aehJrfQwj"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgC61-ah_WIz"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJR_KsIDfW_u"
      },
      "source": [
        "dataset = pd.read_csv('50_Startups.csv')\r\n",
        "X = dataset.iloc[:, :-1].values\r\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV4vkV-IMceb",
        "outputId": "99962c76-548a-4525-d5d7-7828dc53f0d0"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[165349.2 136897.8 471784.1 'New York']\n",
            " [162597.7 151377.59 443898.53 'California']\n",
            " [153441.51 101145.55 407934.54 'Florida']\n",
            " [144372.41 118671.85 383199.62 'New York']\n",
            " [142107.34 91391.77 366168.42 'Florida']\n",
            " [131876.9 99814.71 362861.36 'New York']\n",
            " [134615.46 147198.87 127716.82 'California']\n",
            " [130298.13 145530.06 323876.68 'Florida']\n",
            " [120542.52 148718.95 311613.29 'New York']\n",
            " [123334.88 108679.17 304981.62 'California']\n",
            " [101913.08 110594.11 229160.95 'Florida']\n",
            " [100671.96 91790.61 249744.55 'California']\n",
            " [93863.75 127320.38 249839.44 'Florida']\n",
            " [91992.39 135495.07 252664.93 'California']\n",
            " [119943.24 156547.42 256512.92 'Florida']\n",
            " [114523.61 122616.84 261776.23 'New York']\n",
            " [78013.11 121597.55 264346.06 'California']\n",
            " [94657.16 145077.58 282574.31 'New York']\n",
            " [91749.16 114175.79 294919.57 'Florida']\n",
            " [86419.7 153514.11 0.0 'New York']\n",
            " [76253.86 113867.3 298664.47 'California']\n",
            " [78389.47 153773.43 299737.29 'New York']\n",
            " [73994.56 122782.75 303319.26 'Florida']\n",
            " [67532.53 105751.03 304768.73 'Florida']\n",
            " [77044.01 99281.34 140574.81 'New York']\n",
            " [64664.71 139553.16 137962.62 'California']\n",
            " [75328.87 144135.98 134050.07 'Florida']\n",
            " [72107.6 127864.55 353183.81 'New York']\n",
            " [66051.52 182645.56 118148.2 'Florida']\n",
            " [65605.48 153032.06 107138.38 'New York']\n",
            " [61994.48 115641.28 91131.24 'Florida']\n",
            " [61136.38 152701.92 88218.23 'New York']\n",
            " [63408.86 129219.61 46085.25 'California']\n",
            " [55493.95 103057.49 214634.81 'Florida']\n",
            " [46426.07 157693.92 210797.67 'California']\n",
            " [46014.02 85047.44 205517.64 'New York']\n",
            " [28663.76 127056.21 201126.82 'Florida']\n",
            " [44069.95 51283.14 197029.42 'California']\n",
            " [20229.59 65947.93 185265.1 'New York']\n",
            " [38558.51 82982.09 174999.3 'California']\n",
            " [28754.33 118546.05 172795.67 'California']\n",
            " [27892.92 84710.77 164470.71 'Florida']\n",
            " [23640.93 96189.63 148001.11 'California']\n",
            " [15505.73 127382.3 35534.17 'New York']\n",
            " [22177.74 154806.14 28334.72 'California']\n",
            " [1000.23 124153.04 1903.93 'New York']\n",
            " [1315.46 115816.21 297114.46 'Florida']\n",
            " [0.0 135426.92 0.0 'California']\n",
            " [542.05 51743.15 0.0 'New York']\n",
            " [0.0 116983.8 45173.06 'California']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadrvE7s_lS9"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE5XGL2afr30"
      },
      "source": [
        "# One-hot encoding of 'State' column\r\n",
        "# Replace 'State' column with 3 new dummy columns (since we have 3 categories for 'State')\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\r\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdGNWvQM27A",
        "outputId": "096989ed-1ec4-4abc-ff3a-22d1047eba46"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
            " [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
            " [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n",
            " [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
            " [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n",
            " [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n",
            " [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n",
            " [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n",
            " [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n",
            " [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n",
            " [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n",
            " [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n",
            " [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n",
            " [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n",
            " [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n",
            " [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n",
            " [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n",
            " [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n",
            " [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n",
            " [0.0 0.0 1.0 86419.7 153514.11 0.0]\n",
            " [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n",
            " [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n",
            " [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n",
            " [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n",
            " [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n",
            " [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n",
            " [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n",
            " [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n",
            " [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n",
            " [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n",
            " [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n",
            " [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n",
            " [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n",
            " [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n",
            " [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n",
            " [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n",
            " [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n",
            " [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n",
            " [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n",
            " [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n",
            " [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n",
            " [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n",
            " [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n",
            " [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n",
            " [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n",
            " [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n",
            " [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n",
            " [1.0 0.0 0.0 0.0 135426.92 0.0]\n",
            " [0.0 0.0 1.0 542.05 51743.15 0.0]\n",
            " [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmJIlH2mfe68"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKnUtjbqOJ0Y"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50_Jb5qgOOkQ"
      },
      "source": [
        "In Multiple Linear Regression, even if some features have higher values than others, the coefficients will compensate to put every feature on the same scale. Hence, Feature Scaling is not required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-McZVsQBINc"
      },
      "source": [
        "## Training the Multiple Linear Regression model on the Training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNkXL1YQBiBT"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    }
  ]
}