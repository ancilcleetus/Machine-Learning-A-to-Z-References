{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regr_performance_evaluation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeamvpPVXuS_"
      },
      "source": [
        "# Regression Toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl6BzO1e7tBX"
      },
      "source": [
        "Now we have a complete Regression Toolkit which gives us a lot of different options for future machine learning problems. Now, we can focus on how to use this Regression Toolkit efficiently. First, our focus will be on the performance evaluation of a Regression Model. After this, we will focus on selection of the best Regression Model for any given dataset.\r\n",
        "\r\n",
        "Performance Evaluation of a Regression Model can be done using:\r\n",
        "\r\n",
        "1.   $R^2$ Measure\r\n",
        "2.   Adjusted $R^2$ Measure\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7b8JSF7Adq"
      },
      "source": [
        "## Intuition behind $R^2$ Measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivIZvKadD9f3"
      },
      "source": [
        "Taking Simple Linear Regression as an example, we can see how the model finds the best fit line by minimizing the sum of squares of deviations. This sum of squares is denoted as Sum of Squares of Residuals, $SS_{res}$.\r\n",
        "\r\n",
        "![R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-01.PNG)\r\n",
        "\r\n",
        "For finding the $R^2$ Measure, an average line (horizontal line corresponding to average salary across all observations) is drawn. The sum of squares of deviations from this average line is found and is denoted as Total Sum of Squares, $SS_{tot}$. The average line is a horizontal trend line, but we can think of it as a model fitted to our dataset, but it is not the best model.\r\n",
        "\r\n",
        "![R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-02.PNG)\r\n",
        "\r\n",
        "The Regression Model is trying to fit the best possible line to minimize $SS_{res}$ to make it as small as possible.\r\n",
        "$R^2$ value tells us how good our fitted line is compared to the average line. In the ideal scenario, if $SS_{res}$ = 0 (i.e. fitted line goes through all points in the dataset), $R^2$ = 1. The closer $R^2$ gets to 1, the better our model will be.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5dsSV-yPhQS"
      },
      "source": [
        "## Intuition behind Adjusted $R^2$ Measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAByzWc6Pmif"
      },
      "source": [
        "Lets say we already have a Regression Model with two Features. We now want to add more variables to our model to make it better.\r\n",
        "\r\n",
        "![Adjusted R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-03.PNG)\r\n",
        "\r\n",
        "Adding a new variable can lead to either $R^2$ increasing or remaining the same. But $R^2$ will never decrease.\r\n",
        "Even if the added variable doesn't provide any actual improvement to the model other than some random correlation, $R^2$ may increase. Because of this bias in $R^2$ (it always increases regardless of actual improvement), we will never know whether added variables are actually helping the model or not. So, we need a new parameter to measure the goodness of fit of a model. This is where Adjusted $R^2$ comes into picture.\r\n",
        "\r\n",
        "![R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-04.PNG)\r\n",
        "\r\n",
        "Adjusted $R^2$ has a Penalization Factor. It penalizes you for adding independent variables that don't help your model. As you add more regressors (i.e. independent variables), Adjusted $R^2$ decreases on one hand due to increase in $p$ and on the other hand Adjusted $R^2$ increases due to increase in $R^2$. So, if independent variable doesn't help the model, increase in $R^2$ will be minimal and its effect on Adjusted $R^2$ will be less compared to increase in $p$. This leads to a resultant decrease in Adjusted $R^2$, thus penalizing the added variable. If on the other hand, if added independent variable is helping the model a lot, there will be a significant increase in $R^2$ and its effect on Adjusted $R^2$ will be more compared to increase in $p$. This will lead to a resultant increase in Adjusted $R^2$, overwhelming the Penalizing Factor.\r\n",
        "\r\n",
        "Thus Adjusted $R^2$ is a very good metric that helps in understanding whether you are adding good variables to a model or not."
      ]
    }
  ]
}