{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regr_performance_evaluation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeamvpPVXuS_"
      },
      "source": [
        "# Regression Toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl6BzO1e7tBX"
      },
      "source": [
        "Now we have a complete Regression Toolkit which gives us a lot of different options for future machine learning problems. Now, we can focus on how to use this Regression Toolkit efficiently. First, our focus will be on the performance evaluation of a Regression Model. After this, we will focus on selection of the best Regression Model for any given dataset.\r\n",
        "\r\n",
        "Performance Evaluation of a Regression Model can be done using:\r\n",
        "\r\n",
        "1.   $R^2$ Measure\r\n",
        "2.   Adjusted $R^2$ Measure\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7b8JSF7Adq"
      },
      "source": [
        "## Intuition behind $R^2$ Measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivIZvKadD9f3"
      },
      "source": [
        "Taking Simple Linear Regression as an example, we can see how the model finds the best fit line by minimizing the sum of squares of deviations. This sum of squares is denoted as Sum of Squares of Residuals, $SS_{res}$.\r\n",
        "\r\n",
        "![R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-01.PNG)\r\n",
        "\r\n",
        "For finding the $R^2$ Measure, an average line (horizontal line corresponding to average salary across all observations) is drawn. The sum of squares of deviations from this average line is found and is denoted as Total Sum of Squares, $SS_{tot}$. The average line is a horizontal trend line, but we can think of it as a model fitted to our dataset, but it is not the best model.\r\n",
        "\r\n",
        "![R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-02.PNG)\r\n",
        "\r\n",
        "The Regression Model is trying to fit the best possible line to minimize $SS_{res}$ to make it as small as possible.\r\n",
        "$R^2$ value tells us how good our fitted line is compared to the average line. In the ideal scenario, if $SS_{res}$ = 0 (i.e. fitted line goes through all points in the dataset), $R^2$ = 1. The closer $R^2$ gets to 1, the better our model will be.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5dsSV-yPhQS"
      },
      "source": [
        "## Intuition behind Adjusted $R^2$ Measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAByzWc6Pmif"
      },
      "source": [
        "Lets say we already have a Regression Model with two Features. We now want to add more variables to our model to make it better.\r\n",
        "\r\n",
        "![Adjusted R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-03.PNG)\r\n",
        "\r\n",
        "Adding a new variable can lead to either $R^2$ increasing or remaining the same. But $R^2$ will never decrease.\r\n",
        "Even if the added variable doesn't provide any actual improvement to the model other than some random correlation, $R^2$ may increase. Because of this bias in $R^2$ (it always increases regardless of actual improvement), we will never know whether added variables are actually helping the model or not. So, we need a new parameter to measure the goodness of fit of a model. This is where Adjusted $R^2$ comes into picture.\r\n",
        "\r\n",
        "![R-Squared Measure - Intuition](07-Regr-Performance-Evaluation-04.PNG)\r\n",
        "\r\n",
        "Adjusted $R^2$ has a Penalization Factor. It penalizes you for adding independent variables that don't help your model. As you add more regressors (i.e. independent variables), Adjusted $R^2$ decreases on one hand due to increase in $p$ and on the other hand Adjusted $R^2$ increases due to increase in $R^2$. So, if independent variable doesn't help the model, increase in $R^2$ will be minimal and its effect on Adjusted $R^2$ will be less compared to increase in $p$. This leads to a resultant decrease in Adjusted $R^2$, thus penalizing the added variable. If on the other hand, if added independent variable is helping the model a lot, there will be a significant increase in $R^2$ and its effect on Adjusted $R^2$ will be more compared to increase in $p$. This will lead to a resultant increase in Adjusted $R^2$, overwhelming the Penalizing Factor.\r\n",
        "\r\n",
        "Thus Adjusted $R^2$ is a very good metric that helps in understanding whether you are adding good variables to a model or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOl9H2fNvvSN"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7ZZS1r4gsN"
      },
      "source": [
        "We have a dataset containing salary data of employees of different position levels (1 to 10) in a company. Here,\r\n",
        "\r\n",
        "\r\n",
        "*   Independent Variable/Feature = Level\r\n",
        "*   Dependent/Target Variable = Salary\r\n",
        "\r\n",
        "Here, Salary and Level have non-linear relationship between them. We want to build a model that can predict the salary given the level of an employee.\r\n",
        "\r\n",
        "**Important Note:** Random Forest Regression works well with complex datasets having multiple features. But it is really not the best adapted to 2D datasets with only one Feature and one Target Variable. Here, we are using a simple dataset with only one feature in order to visualize both X and y in a 2D plot. Hence this dataset is not ideal for visualizing the merits of Random Forest Regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2wvZ7SKXzVC"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgbK_F8-X7em"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYD_wROl0kU_"
      },
      "source": [
        "Here we have no missing data in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRlmPQ3taTJ"
      },
      "source": [
        "**Important Note:** If missing data acounts for less than 1% of dataset, we can discard them. But in all other cases, we have to replace missing data. Missing data can be replaced with either mean, median, most frequent data or with a constant using `SimpleImputer` from `sklearn.impute`. Other solutions include `IterativeImputer`, `KNNImputer` and `MissingIndicator`.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadrvE7s_lS9"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25WpFX0fUho"
      },
      "source": [
        "If any of the columns have categorical data, you should apply Encoding to convert them into numerical data. Encoding should be\r\n",
        "\r\n",
        "*   One Hot Encoding if you know there is no ordered relationship in your categorical variable (eg: Country, State etc.)\r\n",
        "*   Label Encoding if there is an ordered relationship (eg: Position Levels in a company, Purchase Decisions etc.)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZqVOmavjptP"
      },
      "source": [
        "In the dataset, you can see that 'Position' column is effectively encoded in 'Level' column. Thus, no explicit encoding is required. Also, since 'Position' column is redundant, it is not included in the Feature Matrix X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKEbUju49M__"
      },
      "source": [
        "Here, we want to predict the salary for Employee Level (which is a continuous real value that varies from 1 to 10). Hence we want the data for all levels from 1 to 10 for accurate prediction. Hence, in this scenario, we are not splitting the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8FeLHYS-nI"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db3wDMTl8eSg"
      },
      "source": [
        "Since predictions from Decision Tree Regression Model or Random Forest Regression Model are resulting from successive splits of the dataset, they both don't require Feature Scaling. Hence, it is not applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4S2fyIBYDcu"
      },
      "source": [
        "## Training the Random Forest Regression model on the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IbsXbK3YM4M"
      },
      "source": [
        "## Predicting a new result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLqF9yMbYTon"
      },
      "source": [
        "## Visualising the Random Forest Regression results (higher resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwie_edtcoGX"
      },
      "source": [
        "Here, you can see that the no: of steps between each position level has increased to 2 compared to 1 in Decision Tree Regression Model. This is because of the use of a forest of trees in the Random Forest Regression Model."
      ]
    }
  ]
}