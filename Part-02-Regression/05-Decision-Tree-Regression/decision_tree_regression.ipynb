{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_tree_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3cas2_1T98w"
      },
      "source": [
        "# Decision Tree Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7b8JSF7Adq"
      },
      "source": [
        "## Intuition behind Support Vector Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esdxurp4Al1c"
      },
      "source": [
        "Decision Tree Regression is a subset of CART (Classification and Regression Trees).\r\n",
        "\r\n",
        "![Decision Tree Regression - Intuition](Decision-Tree-Regression-Intuition-01.PNG)\r\n",
        "\r\n",
        "Let our dataset contains two features X1, X2 and Target Variable y. X1 and X2 can be represented using a scatter plot as below. Notice that y falls in the 3rd dimension.\r\n",
        "\r\n",
        "![Decision Tree Regression - Dataset](Decision-Tree-Regression-Intuition-02.PNG)\r\n",
        "\r\n",
        "Decision Tree Regression involves an optimal split of dataset. Each of these splits are called \"Leaves\". How and where these splits are conducted is determined by the algorithm based on \"Information Entropy\". We then take averages of y for each of these terminal leaves.\r\n",
        "\r\n",
        "![Decision Tree Regression - Splitting & Averaging](Decision-Tree-Regression-Intuition-03.PNG)\r\n",
        "\r\n",
        "The predicted y for a new data will be the average for the corresponding leaf to which the new data falls.\r\n",
        "\r\n",
        "![Decision Tree Regression - Prediction](Decision-Tree-Regression-Intuition-04.PNG)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7Z8unyNNDx"
      },
      "source": [
        "**Important Note:** The mathematical background of Decision Tree Regression is based on Information Entropy. Since our algorithm takes care of optimal splitting of dataset, Information Entropy need not be discussed now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOl9H2fNvvSN"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7ZZS1r4gsN"
      },
      "source": [
        "We have a dataset containing salary data of employees of different position levels (1 to 10) in a company. Here,\r\n",
        "\r\n",
        "\r\n",
        "*   Independent Variable/Feature = Level\r\n",
        "*   Dependent/Target Variable = Salary\r\n",
        "\r\n",
        "Here, Salary and Level have non-linear relationship between them. We want to build a model that can predict the salary given the level of an employee.\r\n",
        "\r\n",
        "**Important Note:** Decision Tree Regression works well with complex datasets having multiple features. Here, we are using a simple dataset with only one feature and hence this dataset is not ideal for visualizing the merits of Decision Tree Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IODliia6U1xO"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y98nA5UdU6Hf"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpjZ43YlU8eI"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVaXoYVU_Uy"
      },
      "source": [
        "dataset = pd.read_csv('Position_Salaries.csv')\n",
        "X = dataset.iloc[:, 1:-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYD_wROl0kU_"
      },
      "source": [
        "Here we have no missing data in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRlmPQ3taTJ"
      },
      "source": [
        "**Important Note:** If missing data acounts for less than 1% of dataset, we can discard them. But in all other cases, we have to replace missing data. Missing data can be replaced with either mean, median, most frequent data or with a constant using `SimpleImputer` from `sklearn.impute`. Other solutions include `IterativeImputer`, `KNNImputer` and `MissingIndicator`.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadrvE7s_lS9"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuwnPMCU7fHk"
      },
      "source": [
        "In the dataset, you can see that 'Position' column is effectively encoded in 'Level' column. Hence, 'Position' column is redundant and is not included in the Feature Matrix X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKEbUju49M__"
      },
      "source": [
        "Here, we want to predict the salary for Employee Level (which is a continuous real value that varies from 1 to 10). Hence we want the data for all levels from 1 to 10 for accurate prediction. Hence, in this scenario, we are not splitting the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8FeLHYS-nI"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db3wDMTl8eSg"
      },
      "source": [
        "Decision Tree Regression doesn't require Feature Scaling. Hence, it is not applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g16qFkFQVC35"
      },
      "source": [
        "## Training the Decision Tree Regression model on the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRGPTH3VcOn"
      },
      "source": [
        "## Predicting a new result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph8ExBj0VkIT"
      },
      "source": [
        "## Visualising the Decision Tree Regression results (higher resolution)"
      ]
    }
  ]
}